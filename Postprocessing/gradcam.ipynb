{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import keras_cv_attention_models\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyper parameters:\n",
    "image_size = 150 \n",
    "batch_size =100 \n",
    "learning_rate = 0.001 \n",
    "image_count_thr = 20 \n",
    "opt ='Adam'\n",
    "seed=42\n",
    "modell = \"MobileViT_S\" \n",
    "visit = \"0912\" \n",
    "cv = \"cv1\" \n",
    "cv_rate = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"/home/yebi/ComputerVision_PLF/Pig_BW/Pig_BW_DL_beta/DL/\")\n",
    "df1 = pd.read_csv(\"./labelled_depth_0718.csv\", dtype={'Visit': str})\n",
    "df2 = pd.read_csv(\"./labelled_depth_0801.csv\", dtype={'Visit': str})\n",
    "df3 = pd.read_csv(\"./labelled_depth_0815.csv\", dtype={'Visit': str})\n",
    "df4 = pd.read_csv(\"./labelled_depth_0829.csv\", dtype={'Visit': str})\n",
    "df5 = pd.read_csv(\"./labelled_depth_0912.csv\", dtype={'Visit': str})\n",
    "df6 = pd.read_csv(\"./labelled_depth_0927.csv\", dtype={'Visit': str})\n",
    "\n",
    "# # labelled_depth = pd.concat([df1, df2, df3], axis=0)\n",
    "if cv == 'cv1':\n",
    "    if visit == \"0718\":\n",
    "        labelled_depth = df1\n",
    "    elif visit == \"0801\":\n",
    "        labelled_depth = df2\n",
    "    elif visit == \"0815\":\n",
    "        labelled_depth = df3\n",
    "    elif visit == \"0829\":\n",
    "        labelled_depth = df4\n",
    "    elif visit == \"0912\":\n",
    "        labelled_depth = df5\n",
    "    elif visit == \"0927\":\n",
    "        labelled_depth = df6\n",
    "   \n",
    "if cv == \"cv2\" or cv == 'cv2_0':\n",
    "    if visit == \"0801\":\n",
    "        labelled_depth = pd.concat([df1, df2], axis=0)\n",
    "    elif visit == \"0815\":\n",
    "        labelled_depth = pd.concat([df1, df2, df3], axis=0)\n",
    "    elif visit == \"0829\":\n",
    "        labelled_depth = pd.concat([df1, df2, df3, df4], axis=0)\n",
    "    elif visit == \"0912\":\n",
    "        labelled_depth = pd.concat([df1, df2, df3, df4, df5], axis=0)\n",
    "    elif visit == \"0927\":\n",
    "        labelled_depth = pd.concat([df1, df2, df3, df4, df5, df6], axis=0)\n",
    "\n",
    "######################################################################\n",
    "## Remove outliers in ground truth body weights.\n",
    "weight_percentile = 3\n",
    "weight_threshold = labelled_depth[\"Weights\"].quantile(weight_percentile / 100)\n",
    "labelled_depth = labelled_depth[labelled_depth[\"Weights\"] >= weight_threshold]\n",
    "print(f\"Remove weight outliers by {weight_percentile}% quantile\")\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Count the occurrences of each category\n",
    "category_counts = labelled_depth['Bag_ID'].value_counts()\n",
    "# Select rows based on the frequency condition\n",
    "selected_rows = pd.DataFrame()\n",
    "for category, count in category_counts.items():\n",
    "    category_data = labelled_depth[labelled_depth['Bag_ID'] == category]\n",
    "    if count <= image_count_thr:\n",
    "        selected_rows = pd.concat([selected_rows, category_data])\n",
    "    else:\n",
    "        interval = count // image_count_thr\n",
    "        selected_indices = np.arange(0, count, interval)[:image_count_thr]\n",
    "        selected_rows = pd.concat([selected_rows, category_data.iloc[selected_indices]])\n",
    "labelled_depth = selected_rows\n",
    "print(\"Total images are \", labelled_depth.shape[0])\n",
    "\n",
    "######################################################################\n",
    "\n",
    "def read_images(labelled_depth, image_size):\n",
    "    images = []\n",
    "    img_paths = []\n",
    "    for filename in labelled_depth[\"FilePath\"]:\n",
    "        if filename.endswith('.png'):  \n",
    "            if os.path.exists(filename):\n",
    "                img_path = filename\n",
    "                img = tf.io.read_file(img_path)\n",
    "                img = tf.image.decode_png(img, channels=3)  \n",
    "                img = tf.image.resize_with_crop_or_pad(img, target_height=int(img.shape[1]), target_width=int(img.shape[1]))\n",
    "                img = tf.image.resize(img, [image_size, image_size]) #Resize images\n",
    "                images.append(img)\n",
    "                img_paths.append(img_path)\n",
    "\n",
    "    processed_images = tf.stack(images)\n",
    "    processed_images = tf.cast(processed_images, tf.float32)\n",
    "    processed_images /= 255.0\n",
    "    print(\"DDDDDDDDDDDDDDDDDDDDDDDD\")\n",
    "    return processed_images, img_paths\n",
    "######################################################################\n",
    "\n",
    "if cv == \"cv1\":\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    pig_n = np.unique(labelled_depth['Bag_ID']).shape[0]\n",
    "    train_bag_id = random.sample(list(np.unique(labelled_depth[\"Bag_ID\"])), int(pig_n*0.8))\n",
    "    train_df = labelled_depth[labelled_depth[\"Bag_ID\"].isin(train_bag_id)]\n",
    "    test_df = labelled_depth[-labelled_depth[\"Bag_ID\"].isin(train_bag_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now is reading testing sets\n",
      "DDDDDDDDDDDDDDDDDDDDDDDD\n",
      "x_test shape is:  (1310, 150, 150, 3) 1310\n",
      "y_test shape is:  (1310,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Now is reading testing sets\")\n",
    "x_test, test_img_path = read_images(labelled_depth=test_df, image_size = image_size)\n",
    "print(\"x_test shape is: \", x_test.shape, len(test_img_path))\n",
    "y_test = test_df[\"Weights\"].values\n",
    "y_test = y_test.astype(np.float32)\n",
    "print(\"y_test shape is: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keras_cv_attention_model(modell):           \n",
    "    if modell == \"MobileViT_XXS\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_XXS(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileViT_S\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_S(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileViT_V2_050\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_V2_050(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileViT_V2_100\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_V2_100(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileViT_V2_150\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_V2_150(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileViT_V2_200\":\n",
    "        base_model = keras_cv_attention_models.mobilevit.MobileViT_V2_200(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileNetV3Small050\":\n",
    "        base_model = keras_cv_attention_models.mobilenetv3_family.mobilenetv3.MobileNetV3Small050(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileNetV3Large075\":\n",
    "        base_model = keras_cv_attention_models.mobilenetv3_family.mobilenetv3.MobileNetV3Large075(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileNetV3Large100\":\n",
    "        base_model = keras_cv_attention_models.mobilenetv3_family.mobilenetv3.MobileNetV3Large100(pretrained='imagenet', num_classes=0, input_shape=(image_size, image_size, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Model {modell} not recognized in keras_cv_attention_model function\")\n",
    "\n",
    "    return base_model\n",
    "\n",
    "\n",
    "############################\n",
    "def keras_model(modell):\n",
    "    if modell == \"ResNet50\":\n",
    "        base_model = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "    elif modell == \"MobileNet050\":\n",
    "        base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), alpha=0.5)\n",
    "    elif modell == \"MobileNet100\":\n",
    "        base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), alpha=1.0) \n",
    "    elif modell == \"MobileNet075\":\n",
    "        base_model = keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), alpha=0.75)   \n",
    "    else:\n",
    "        raise ValueError(f\"Model {modell} not recognized in keras_model function\")\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    return base_model\n",
    "\n",
    "\n",
    "############################    \n",
    "def load_bw_model(model):\n",
    "    try:\n",
    "        return keras_cv_attention_model(model)\n",
    "    except ValueError:\n",
    "        return keras_model(model)\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/yebi/.keras/models/mobilevit_s_imagenet.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "base_model = load_bw_model(model=modell)\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(1, activation='linear', kernel_regularizer=l2(0.001))(x)\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visit == '0912':\n",
    "    checkpoint_filepath = '/home/yebi/ComputerVision_PLF/Pig_BW/Run/BestModelCV1/T5_Model/checkpoint_MobileViT_S_cv1_0.8_visit_0912_image_size_150_trainable_True_batch_size_100_epochs_300_lr_0.001_seed_42_opt_Adam_image_count_thr_20.h5'\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "if opt == \"Adam\":\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate, epsilon=1e-05)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss=keras.losses.MeanSquaredError(), \n",
    "            metrics=[keras.losses.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_weight = np.squeeze(model.predict(x_test))\n",
    "true_weight = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, output_index=0):\n",
    "    # Create a model that maps the input image to the activations of the last conv layer as well as the output predictions\n",
    "    grad_model = keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Compute the gradient of the output neuron (linear regression) with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        class_channel = preds[:, output_index]\n",
    "\n",
    "    # Get the gradients of the output w.r.t the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # Compute the mean intensity of the gradient over the feature map channels\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Multiply each channel in the feature map array by the importance of this channel\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "layer_names = []\n",
    "\n",
    "# Collect the names of all Conv2D layers in reverse order\n",
    "for layer in reversed(model.layers):\n",
    "    if isinstance(layer, keras.layers.Conv2D):\n",
    "        layer_names.append(layer.name)\n",
    "\n",
    "# Number of Conv2D layers\n",
    "num_layers = len(layer_names)\n",
    "\n",
    "# Number of columns per row\n",
    "cols = 3\n",
    "# Calculate number of rows needed\n",
    "rows = (num_layers + cols - 1) // cols\n",
    "\n",
    "# Create subplots with the determined number of rows and columns\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "\n",
    "# Plot heatmap for each Conv2D layer\n",
    "rr = 200\n",
    "img_path0 = test_img_path[rr]\n",
    "img_array = np.expand_dims(x_test[rr], axis=0)\n",
    "\n",
    "for idx, layer_name in enumerate(layer_names):\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, layer_name)\n",
    "    \n",
    "    # Determine the position in the subplot grid\n",
    "    row = idx // cols\n",
    "    col = idx % cols\n",
    "\n",
    "    # Display heatmap in the respective subplot\n",
    "    axs[row, col].matshow(heatmap)\n",
    "    axs[row, col].set_title(f'Heatmap for layer: {layer_name}')\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(num_layers, rows * cols):\n",
    "    row = idx // cols\n",
    "    col = idx % cols\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "def process_gradcam(img_path11, heatmap, alpha=1):\n",
    "    # Load the original image\n",
    "    img = keras.utils.load_img(img_path11)\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    img = tf.image.resize_with_crop_or_pad(img, target_height=int(img.shape[1]), target_width=int(img.shape[1]))\n",
    "    img = tf.image.resize(img, [image_size, image_size]) #Resize images\n",
    "    img = keras.utils.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use inferno colormap to colorize heatmap\n",
    "    inferno = mpl.colormaps[\"viridis\"]\n",
    "    inferno_colors = inferno(np.arange(256))[:, :3]\n",
    "    inferno_heatmap = inferno_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    inferno_heatmap = keras.utils.array_to_img(inferno_heatmap)\n",
    "    inferno_heatmap = inferno_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    inferno_heatmap = keras.utils.img_to_array(inferno_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on the original image\n",
    "    superimposed_img = inferno_heatmap * alpha + img*0.2\n",
    "    return keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "\n",
    "def display_gradcam_grid(img_paths, heatmaps, labels, groud_bw, rrr, grid_size=(5, 5)):\n",
    "    num_rows, num_cols = grid_size\n",
    "    # Adjust the figure size based on the number of rows and columns\n",
    "    fig_size = (num_cols * 3, num_rows * 3)  # 3 is a scaling factor, adjust as necessary\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=fig_size)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, img_path, heatmap, label, y, r in zip(axes, img_paths, heatmaps, labels, groud_bw, rrr):\n",
    "        img_with_heatmap = process_gradcam(img_path, heatmap)\n",
    "        ax.imshow(img_with_heatmap)\n",
    "        ax.set_title(f\"Measurement Error: {label:.2f} kg\\nScale-BW: {y:.2f} kg\\n #index: {r}\")\n",
    "        # ax.set_title(f\"Measurement Error: {label:.2f} kg\\nScale-BW: {y:.2f} kg\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"./DrawingPlots/grad_cam.pdf\", format='pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Weight</th>\n",
       "      <th>Predicted Weight</th>\n",
       "      <th>ME</th>\n",
       "      <th>Absolute ME</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>121.109001</td>\n",
       "      <td>121.097298</td>\n",
       "      <td>-0.011703</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>106.594002</td>\n",
       "      <td>106.611893</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>0.017891</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>117.933998</td>\n",
       "      <td>117.959900</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>100.244003</td>\n",
       "      <td>100.216293</td>\n",
       "      <td>-0.027710</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>118.388000</td>\n",
       "      <td>118.359047</td>\n",
       "      <td>-0.028954</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>107.955002</td>\n",
       "      <td>132.318405</td>\n",
       "      <td>24.363403</td>\n",
       "      <td>24.363403</td>\n",
       "      <td>1306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>107.955002</td>\n",
       "      <td>133.106018</td>\n",
       "      <td>25.151016</td>\n",
       "      <td>25.151016</td>\n",
       "      <td>1307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>107.955002</td>\n",
       "      <td>134.106110</td>\n",
       "      <td>26.151108</td>\n",
       "      <td>26.151108</td>\n",
       "      <td>1308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>107.955002</td>\n",
       "      <td>134.143234</td>\n",
       "      <td>26.188232</td>\n",
       "      <td>26.188232</td>\n",
       "      <td>1309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>107.955002</td>\n",
       "      <td>134.302780</td>\n",
       "      <td>26.347778</td>\n",
       "      <td>26.347778</td>\n",
       "      <td>1310.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1310 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual Weight  Predicted Weight         ME  Absolute ME    Rank\n",
       "1116     121.109001        121.097298  -0.011703     0.011703     1.0\n",
       "1080     106.594002        106.611893   0.017891     0.017891     2.0\n",
       "777      117.933998        117.959900   0.025902     0.025902     3.0\n",
       "747      100.244003        100.216293  -0.027710     0.027710     4.0\n",
       "1152     118.388000        118.359047  -0.028954     0.028954     5.0\n",
       "...             ...               ...        ...          ...     ...\n",
       "259      107.955002        132.318405  24.363403    24.363403  1306.0\n",
       "258      107.955002        133.106018  25.151016    25.151016  1307.0\n",
       "251      107.955002        134.106110  26.151108    26.151108  1308.0\n",
       "253      107.955002        134.143234  26.188232    26.188232  1309.0\n",
       "254      107.955002        134.302780  26.347778    26.347778  1310.0\n",
       "\n",
       "[1310 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame({\n",
    "    'Actual Weight': y_test,\n",
    "    'Predicted Weight': predicted_weight,\n",
    "    'ME': predicted_weight - y_test\n",
    "})\n",
    "\n",
    "# Calculate the absolute mean error (ME)\n",
    "df_cv['Absolute ME'] = df_cv['ME'].abs()\n",
    "\n",
    "# Sort the DataFrame by the absolute ME\n",
    "df_cv_sorted = df_cv.sort_values(by='Absolute ME')\n",
    "\n",
    "# Assign ranks based on sorted order\n",
    "df_cv_sorted['Rank'] = df_cv_sorted['Absolute ME'].rank(method='min')\n",
    "df_cv_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100 = df_cv_sorted[:150].sort_values(by=\"Actual Weight\")\n",
    "rrr = df100.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "labelss = y_test-predicted_weight\n",
    "\n",
    "img_paths = [test_img_path[i] for i in rrr]\n",
    "heatmaps = [make_gradcam_heatmap(np.expand_dims(x_test[i], axis=0), model, 'stack4_block5_post_2_conv') for i in rrr]\n",
    "# heatmaps = [make_gradcam_heatmap(np.expand_dims(x_test[i], axis=0), model, 'features_conv') for i in rrr]\n",
    "\n",
    "labels = [(round(labelss[i], 2)) for i in rrr]\n",
    "groud_bw = [(round(y_test[i], 2)) for i in rrr]\n",
    "\n",
    "# Calculate grid size\n",
    "num_images = len(heatmaps)\n",
    "cols = 5  # Number of columns (you can adjust this)\n",
    "rows = len(rrr) // cols  # Calculate number of rows needed\n",
    "\n",
    "display_gradcam_grid(img_paths, heatmaps, labels, groud_bw, rrr, grid_size=(rows, cols))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
